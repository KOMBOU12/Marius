{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOMBOU12/Marius/blob/main/MALIASH_2024_Repr%C3%A9sentation_vectorielle_du_sens_des_mots_(Sujet).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MBLqlgL3BYi"
      },
      "source": [
        "# Représentation vectorielle du sens des mots\n",
        "\n",
        "**Auteur :** Adrien Guille (Université Lumière Lyon 2), pour le cours *Representation learning for NLP* @ Master 2 MALIA et Master 2 MIASH."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P1gOVRYXsAP"
      },
      "source": [
        "## Chargement des vecteurs\n",
        "\n",
        "On télécharge une petite collection de vecteurs appris par la méthode **Skip-Gram avec échantillonnage négatif**.\n",
        "\n",
        "Les représentations en dimension 300 des 100 000 mots les plus fréquents ont été apprises à partir de l'ensemble des pages francophones sur Wikipedia. Autrement dit, chacun des 100 000 mots les plus fréquents dans les articles Wikipedia sont associés à un vecteur $u \\in \\mathbb{R}^{300}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h8Tz4x6uzQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe46294-fa4f-4207-c304-718c7615fc96"
      },
      "source": [
        "# Téléchargement\n",
        "! wget https://raw.githubusercontent.com/AdrienGuille/adrienguille.github.io/main/assets/sgns.wiki.fr.zip\n",
        "! unzip sgns.wiki.fr.zip\n",
        "\n",
        "# Lecture du vocabulaire et des vecteurs\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"sgns.wiki.fr.vec\", sep=\" \", quoting=3, header=None, skiprows=1)\n",
        "vocabulary = list(data[0].values)\n",
        "vectors = data[range(1, 301)].values\n",
        "\n",
        "# Définition d'une fonction pour accéder de façon sûre aux vecteurs\n",
        "def get_vector(word):\n",
        "  if word in vocabulary:\n",
        "    return vectors[vocabulary.index(word)]\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-07 05:06:35--  https://raw.githubusercontent.com/AdrienGuille/adrienguille.github.io/main/assets/sgns.wiki.fr.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 98204893 (94M) [application/zip]\n",
            "Saving to: ‘sgns.wiki.fr.zip’\n",
            "\n",
            "sgns.wiki.fr.zip    100%[===================>]  93.66M  43.4MB/s    in 2.2s    \n",
            "\n",
            "2024-12-07 05:06:39 (43.4 MB/s) - ‘sgns.wiki.fr.zip’ saved [98204893/98204893]\n",
            "\n",
            "Archive:  sgns.wiki.fr.zip\n",
            "  inflating: sgns.wiki.fr.vec        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary.index('vice')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7gbdwxcR9hF",
        "outputId": "69dd012b-d5a3-45a4-b519-9d2ef120396f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "948"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors[10].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUu49ssjYZRM",
        "outputId": "79ef2561-4193-4c89-a6c5-93c1f089a5fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnJuZn6YWxMn"
      },
      "source": [
        "## Identification des mots proches\n",
        "\n",
        "On mesure la proximité entre les mots comme une fonction du produit scalaire de leurs représentations. Plus exactement on mesure le produit scalaire normalisé par le produit des normes, qui correspond au cosinus de l'angle entre les deux vecteurs :\n",
        "$$\n",
        "\\cos(\\theta_{u_1, u_2}) = \\frac{u_1 \\cdot u_2}{||u_1|| \\times ||u_2||}\n",
        "$$\n",
        "\n",
        "**Exercice** :\n",
        "- Écrire la fonction `get_most_similar_words` qui reçoit une représentation vectorielle ou un mot et retourne les $n$ mots les plus proches.\n",
        "- La tester !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFbPPqod-USy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e405cd2a-482d-447d-9a50-4878c5e56a68"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import heapq\n",
        "\n",
        "# Fonction pour trouver les mots les plus similaires\n",
        "def get_most_similar_words(vector, n=10):\n",
        "    if isinstance(vector, str):\n",
        "        # Si un mot est donné, récupère son vecteur\n",
        "        vector = get_vector(vector)\n",
        "        if vector is None:\n",
        "            return f\"Le mot '{vector}' n'existe pas dans le vocabulaire.\"\n",
        "\n",
        "    # Liste pour stocker les similarités\n",
        "    similarities = []\n",
        "\n",
        "    # Calcul des similarités cosinus entre le vecteur donné et tous les vecteurs\n",
        "    for i in range(len(vectors)):\n",
        "        sim = cosine_similarity(vector.reshape(1, -1), vectors[i].reshape(1, -1))[0][0]\n",
        "        similarities.append((sim, vocabulary[i]))\n",
        "\n",
        "    # Trouver les n mots les plus proches (tri par similarité décroissante)\n",
        "    most_similar = heapq.nlargest(n, similarities, key=lambda x: x[0])\n",
        "\n",
        "    # Retourne les n mots avec leurs similarités\n",
        "    return [(word, round(sim, 4)) for sim, word in most_similar]\n",
        "\n",
        "# Tester la fonction\n",
        "#print(get_most_similar_words(\"roi\", n=5))\n",
        "print(get_most_similar_words(\"chien\", n=5))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('chien', 1.0), ('chiens', 0.7498), ('chienne', 0.7083), ('chiot', 0.6648), ('chat', 0.663)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W86FJ0b6XNMb"
      },
      "source": [
        "## Visualisation des représentations et des relations entre elles: pays & capitales\n",
        "\n",
        "**Exercice:**\n",
        "- Extraire le sous-ensemble de vecteurs pour les mots donnés ci-après\n",
        "- Les projeter linéairement en 3D au moyen d'une décomposition en valeurs singulières tronquée\n",
        "- Visualiser les vecteurs dans le plan, d'après leurs coordonnées sur les 2ème et 3ème axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzRMB3sYuK3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e98d68f-812c-495c-96b6-9ee6ec2d6611"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "words = ['paris', 'france', 'berlin', 'allemagne', 'pékin', 'chine', 'tokyo', 'japon', 'mexico', 'mexique', 'caracas', 'venezuela']\n",
        "word_vectors = [get_vector(w) for w in words]\n",
        "print(word_vectors[0])\n",
        "\n",
        "for i in range(len(word_vectors)):\n",
        "    if word_vectors[i] is None:\n",
        "        word_vectors[i] = np.zeros(300)  # remplacez par un vecteur de zeros\n",
        "        print(f\"le mot {word} \")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.46809    -0.085333   -0.050467    0.15472    -0.29726    -0.18778\n",
            " -0.38776     0.27708     0.010199    0.43666    -0.25566     0.083927\n",
            " -0.11093    -0.11587     0.14745     0.25013    -0.013815   -0.24469\n",
            "  0.0045887  -0.41337     0.36828    -0.07675    -0.035031   -0.1002\n",
            " -0.12031    -0.13893    -0.076939    0.33799     0.28288    -0.27698\n",
            "  0.20382     0.010743   -0.11533    -0.14541     0.0059088  -0.1152\n",
            " -0.23027     0.27056     0.045689   -0.23194     0.077845    0.019229\n",
            " -0.33749     0.078832   -0.023005   -0.076837    0.039265    0.20436\n",
            " -0.1098      0.44364    -0.24161     0.20541     0.034744    0.13123\n",
            "  0.29599    -0.13054    -0.13865     0.13754     0.33452    -0.30304\n",
            "  0.037831   -0.063212    0.13506     0.10116    -0.056717    0.43578\n",
            " -0.13298     0.30716     0.061135   -0.12346     0.18718     0.022046\n",
            " -0.14464    -0.067695   -0.39739     0.032968    0.026597   -0.26362\n",
            " -0.024051   -0.40203     0.0063361  -0.075203   -0.031082    0.26956\n",
            "  0.035396    0.28499    -0.22048    -0.14329    -0.15248     0.24799\n",
            " -0.042804    0.096656    0.2086     -0.019517    0.026084   -0.049671\n",
            " -0.20474     0.13694    -0.0073943  -0.24124     0.10676     0.027611\n",
            " -0.088997   -0.025988   -0.12699     0.33735    -0.080963   -0.082709\n",
            "  0.0079598   0.18607    -0.22259     0.34021    -0.18431    -0.13294\n",
            "  0.14252    -0.18212     0.065711   -0.049004   -0.29668     0.04521\n",
            " -0.11962    -0.13865     0.14202     0.18115    -0.036925   -0.19281\n",
            "  0.19748     0.13083    -0.020983    0.42904    -0.10135     0.018601\n",
            "  0.55523     0.059822   -0.077625   -0.010913   -0.11513     0.044875\n",
            " -0.36755    -0.23829     0.13163    -0.32482     0.077016   -0.34002\n",
            "  0.15551     0.1737      0.098608    0.049887    0.19961    -0.10554\n",
            " -0.23899     0.098009   -0.11214     0.28671    -0.46025     0.11127\n",
            "  0.19002     0.27503     0.093726    0.068603   -0.36612    -0.10797\n",
            "  0.1056      0.23608    -0.049125    0.37555     0.087347    0.033019\n",
            "  0.04239    -0.29567     0.011809   -0.32338     0.024942   -0.023165\n",
            "  0.10686    -0.29059    -0.083744    0.017453    0.28517    -0.070611\n",
            " -0.33373    -0.54079    -0.011327   -0.17845     0.36006     0.41878\n",
            " -0.19641    -0.19924     0.36137    -0.43636    -0.23365     0.068571\n",
            " -0.13638     0.04121     0.12244    -0.16043     0.18692     0.20276\n",
            " -0.44707    -0.084135    0.3766      0.31071     0.048757   -0.13996\n",
            "  0.10958     0.063375   -0.081383    0.083578   -0.23804     0.1322\n",
            "  0.42685    -0.059757    0.15303     0.024695   -0.28472    -0.11644\n",
            " -0.030186   -0.20632    -0.26507     0.28991    -0.7252      0.25708\n",
            " -0.27518    -0.06105     0.058075   -0.13233     0.15128     0.0012935\n",
            "  0.070889    0.072284    0.33924    -0.025093    0.36333    -0.0070304\n",
            "  0.096001    0.26084     0.29457    -0.076552   -0.39423    -0.15953\n",
            " -0.12708     0.10374     0.10539     0.41063    -0.16024    -0.38787\n",
            " -0.073835   -0.13926    -0.0029368   0.034197   -0.17596     0.20254\n",
            " -0.099732   -0.093919   -0.0023443  -0.07719    -0.21434    -0.2603\n",
            " -0.15451    -0.21369     0.19608    -0.61736     0.10807     0.41482\n",
            "  0.0095963  -0.025632   -0.028494   -0.47416     0.043478   -0.28471\n",
            "  0.24796    -0.10263     0.21305     0.08177    -0.044509   -0.0055738\n",
            "  0.19605     0.074503   -0.00083199 -0.23579     0.15302    -0.18847\n",
            "  0.17001    -0.12827     0.33676     0.30131    -0.19819     0.043791\n",
            " -0.097946   -0.25765    -0.26348     0.22558     0.13369     0.085276\n",
            " -0.32582     0.13493    -0.039556    0.29641     0.016042    0.13332   ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aD6U1rfXdgt"
      },
      "source": [
        "## Prédiction du nom de la capitale à partir du nom d'un pays\n",
        "\n",
        "**Exercice** :\n",
        "- D'après le résultat obtenu précédemment, proposer une manière de deviner le nom de la capitale à partir du nom d'un pays d'après leurs représentations.\n",
        "- Écrire la fonction `find_capital` qui implémente votre idée et la tester !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld4s9xOy6p2r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLSEVn49XjGL"
      },
      "source": [
        "## Résolution d'analogies\n",
        "\n",
        "On considère des analogies de la forme $a$ est à $b$, ce que $c$ est à $d$. Dans l'espace vectoriel on devrait avoir $u_a - u_b = u_c - u_d$.\n",
        "\n",
        "**Exercice** :\n",
        "- Écrire une fonction qui résoud une telle analogie, sachant $a$, $b$ et $c$.\n",
        "- La tester !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1J-rnsDyCSU"
      },
      "source": [
        "def solve_analogy(word_a, word_b, word_c):\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}