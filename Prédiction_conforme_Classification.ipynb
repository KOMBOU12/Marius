{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOELPOtMc+dE+XzqQJpLs7S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOMBOU12/Marius/blob/main/Pr%C3%A9diction_conforme_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Projet : Classification et  Prédiction conforme**\n",
        "Le jeu de données **CIFAR-10** contient 60 000 images en couleur de taille 32x32. Ces images représentent des objets appartenant à 10 classes :\n",
        "\n",
        "*   avion\n",
        "*   automobile\n",
        "*   oiseau\n",
        "*   chat\n",
        "*   cerf\n",
        "*   chien\n",
        "*   grenouille\n",
        "*   cheval\n",
        "*   bateau\n",
        "*   camion\n",
        "\n",
        "Le jeu de données d'entraînement contient 50 000 images d'entraînement, soit 5 000 images par classe. Le jeu de test contient 10 000 images de test, soit 1 000 images par classe."
      ],
      "metadata": {
        "id": "TzA4AfATQeoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pourquoi ce jeu de donnée ?**\n",
        "\n",
        "CIFAR-10 comme jeu de données est un bon point de départ pour entraîner et tester des Convolutional Neural Networks (CNN), car il n'est pas trop volumineux mais suffisamment varié pour évaluer la performance d'un modèle."
      ],
      "metadata": {
        "id": "lcDlG89ZlEqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importation des données et statistiques simples**\n",
        "\n",
        "Tu dois dire pourquoi tu choisis ce jeu de donnée. Tu dois également faire de la regression linéaire sur ce jeu de donnée et dire si ça marche ou pas et pourquoi. Vu que tu as une CNN tu peux peut être penser faire de la CV+,  pour faire de la prédiction conforme. Il semble que la SCP serait judicieux"
      ],
      "metadata": {
        "id": "m3GUMGrrRuv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger les données CIFAR-10\n",
        "(X_full, y_full), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Diviser les données en trois ensembles : entraînement, calibration et test\n",
        "X_train, X_calibration, y_train, y_calibration = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalisation des données\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_calibration = X_calibration.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "XRZhWM4FFXl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Entraîner le modèle sur l'ensemble d'entraînement\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Prédire sur l'ensemble de calibration\n",
        "y_pred_calibration = model.predict(X_calibration)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3z7fP0_FKMb",
        "outputId": "e9987c0e-8dff-453f-9875-8b5957ae9b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 79ms/step - accuracy: 0.3278 - loss: 1.8394 - val_accuracy: 0.4893 - val_loss: 1.4549\n",
            "Epoch 2/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 80ms/step - accuracy: 0.5255 - loss: 1.3333 - val_accuracy: 0.5548 - val_loss: 1.2339\n",
            "Epoch 3/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 77ms/step - accuracy: 0.5892 - loss: 1.1666 - val_accuracy: 0.6060 - val_loss: 1.1226\n",
            "Epoch 4/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 77ms/step - accuracy: 0.6307 - loss: 1.0496 - val_accuracy: 0.6245 - val_loss: 1.0843\n",
            "Epoch 5/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 77ms/step - accuracy: 0.6660 - loss: 0.9694 - val_accuracy: 0.6410 - val_loss: 1.0215\n",
            "Epoch 6/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 85ms/step - accuracy: 0.6900 - loss: 0.8993 - val_accuracy: 0.6570 - val_loss: 0.9817\n",
            "Epoch 7/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 86ms/step - accuracy: 0.7078 - loss: 0.8461 - val_accuracy: 0.6545 - val_loss: 0.9899\n",
            "Epoch 8/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 78ms/step - accuracy: 0.7188 - loss: 0.8027 - val_accuracy: 0.6762 - val_loss: 0.9483\n",
            "Epoch 9/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 89ms/step - accuracy: 0.7406 - loss: 0.7595 - val_accuracy: 0.6733 - val_loss: 0.9737\n",
            "Epoch 10/10\n",
            "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 86ms/step - accuracy: 0.7494 - loss: 0.7196 - val_accuracy: 0.6740 - val_loss: 0.9549\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **La prédiction conforme**\n",
        "\n",
        "On fera une SCP parceque nous avons beaucoup de données."
      ],
      "metadata": {
        "id": "xXmNQwr-jKjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Calculer les scores de conformité sur l'ensemble de calibration\n",
        "def calculer_scores_conformite(y_pred, y_true):\n",
        "    scores_conformite = []\n",
        "    for i in range(len(y_true)):\n",
        "        prob_vraie_classe = y_pred[i][y_true[i][0]]\n",
        "        score = 1 - prob_vraie_classe\n",
        "        scores_conformite.append(score)\n",
        "    return np.array(scores_conformite)\n",
        "\n",
        "scores_conformite_calibration = calculer_scores_conformite(y_pred_calibration, y_calibration)\n",
        "\n",
        "# Définir un seuil basé sur les scores de calibration et le niveau de confiance\n",
        "alpha = 0.05\n",
        "seuil = np.percentile(scores_conformite_calibration, 100 * (1 - alpha))\n",
        "\n",
        "# Appliquer ce seuil à l'ensemble de test\n",
        "y_pred_test = model.predict(X_test)\n",
        "pred_intervals = [np.where(p >= 1 - seuil)[0] for p in y_pred_test]\n",
        "\n",
        "# Afficher quelques résultats de l'ensemble de test\n",
        "for i in range(5):\n",
        "    print(f\"Image {i+1}: Intervalles de prédiction {pred_intervals[i]}, Classe réelle: {y_test[i][0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu8tEvyRFfnS",
        "outputId": "ff998a6b-4935-4bc2-8f0c-cc101083e23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step\n",
            "Image 1: Intervalles de prédiction [0 3 5], Classe réelle: 3\n",
            "Image 2: Intervalles de prédiction [0 1 8], Classe réelle: 8\n",
            "Image 3: Intervalles de prédiction [0 1 8 9], Classe réelle: 8\n",
            "Image 4: Intervalles de prédiction [0 8], Classe réelle: 0\n",
            "Image 5: Intervalles de prédiction [2 3 4 6], Classe réelle: 6\n"
          ]
        }
      ]
    }
  ]
}