{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj1Xmxm2v3UbJLgEiLCGB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOMBOU12/Marius/blob/main/Representation_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cellule de texte <1MBLqlgL3BYi>\n",
        "# %% [markdown]\n",
        "# Représentation vectorielle du sens des mots\n",
        "\n",
        "**Auteur :** Adrien Guille (Université Lumière Lyon 2), pour le cours *Representation learning for NLP* @ Master 2 MALIA et Master 2 MIASH.\n",
        "\n",
        "Cellule de texte <9P1gOVRYXsAP>\n",
        "# %% [markdown]\n",
        "## Chargement des vecteurs\n",
        "\n",
        "On télécharge une petite collection de vecteurs appris par la méthode **Skip-Gram avec échantillonnage négatif**.\n",
        "\n",
        "Les représentations en dimension 300 des 100 000 mots les plus fréquents ont été apprises à partir de l'ensemble des pages francophones sur Wikipedia. Autrement dit, chacun des 100 000 mots les plus fréquents dans les articles Wikipedia sont associés à un vecteur $u \\in \\mathbb{R}^{300}$.\n",
        "\n",
        "Cellule de code <9h8Tz4x6uzQW>"
      ],
      "metadata": {
        "id": "_c8tCmYwHk3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "# Téléchargement\n",
        "! wget https://raw.githubusercontent.com/AdrienGuille/adrienguille.github.io/main/assets/sgns.wiki.fr.zip\n",
        "! unzip sgns.wiki.fr.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjSNurEiHsxF",
        "outputId": "547491ff-3fa6-4021-8b34-dc3b62b2306f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-06 22:37:34--  https://raw.githubusercontent.com/AdrienGuille/adrienguille.github.io/main/assets/sgns.wiki.fr.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 98204893 (94M) [application/zip]\n",
            "Saving to: ‘sgns.wiki.fr.zip’\n",
            "\n",
            "sgns.wiki.fr.zip    100%[===================>]  93.66M   196MB/s    in 0.5s    \n",
            "\n",
            "2024-12-06 22:37:35 (196 MB/s) - ‘sgns.wiki.fr.zip’ saved [98204893/98204893]\n",
            "\n",
            "Archive:  sgns.wiki.fr.zip\n",
            "  inflating: sgns.wiki.fr.vec        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICc3yiUcEB41"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Lecture du vocabulaire et des vecteurs\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"sgns.wiki.fr.vec\", sep=\" \", quoting=3, header=None, skiprows=1)\n",
        "vocabulary = list(data[0].values)\n",
        "vectors = data[range(1, 301)].values\n",
        "\n",
        "# Définition d'une fonction pour accéder de façon sûre aux vecteurs\n",
        "def get_vector(word):\n",
        "  if word in vocabulary:\n",
        "    return vectors[vocabulary.index(word)]\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "Cellule de texte <xnJuZn6YWxMn>\n",
        "# %% [markdown]\n",
        "## Identification des mots proches\n",
        "\n",
        "On mesure la proximité entre les mots comme une fonction du produit scalaire de leurs représentations. Plus exactement on mesure le produit scalaire normalisé par le produit des normes, qui correspond au cosinus de l'angle entre les deux vecteurs :\n",
        "$$\n",
        "\\cos(\\theta_{u_1, u_2}) = \\frac{u_1 \\cdot u_2}{||u_1|| \\times ||u_2||}\n",
        "$$\n",
        "\n",
        "**Exercice** :\n",
        "- Écrire la fonction `get_most_similar_words` qui reçoit une représentation vectorielle ou un mot et retourne les $n$ mots les plus proches.\n",
        "- La tester !\n",
        "\n",
        "Cellule de code <EFbPPqod-USy>\n",
        "# %% [code]\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def get_most_similar_words(vector, n=10):\n",
        "    if type(vector) == str:\n",
        "        # si l'argument reçu est de type texte, on le remplace par le vecteur qui correspond\n",
        "        vector = get_vector(vector)\n",
        "\n",
        "\n",
        "Cellule de texte <W86FJ0b6XNMb>\n",
        "# %% [markdown]\n",
        "## Visualisation des représentations et des relations entre elles: pays & capitales\n",
        "\n",
        "**Exercice:**\n",
        "- Extraire le sous-ensemble de vecteurs pour les mots donnés ci-après\n",
        "- Les projeter linéairement en 3D au moyen d'une décomposition en valeurs singulières tronquée\n",
        "- Visualiser les vecteurs dans le plan, d'après leurs coordonnées sur les 2ème et 3ème axes.\n",
        "\n",
        "Cellule de code <DzRMB3sYuK3J>\n",
        "# %% [code]\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "words = ['paris', 'france', 'berlin', 'allemagne', 'pékin', 'chine', 'tokyo', 'japon', 'mexico', 'mexique', 'caracas', 'venezuela']\n",
        "word_vectors = [get_vector(w) for w in words]\n",
        "\n",
        "\n",
        "Cellule de texte <5aD6U1rfXdgt>\n",
        "# %% [markdown]\n",
        "## Prédiction du nom de la capitale à partir du nom d'un pays\n",
        "\n",
        "**Exercice** :\n",
        "- D'après le résultat obtenu précédemment, proposer une manière de deviner le nom de la capitale à partir du nom d'un pays d'après leurs représentations.\n",
        "- Écrire la fonction `find_capital` qui implémente votre idée et la tester !\n",
        "\n",
        "Cellule de code <Ld4s9xOy6p2r>\n",
        "# %% [code]\n",
        "\n",
        "\n",
        "Cellule de texte <iLSEVn49XjGL>\n",
        "# %% [markdown]\n",
        "## Résolution d'analogies\n",
        "\n",
        "On considère des analogies de la forme $a$ est à $b$, ce que $c$ est à $d$. Dans l'espace vectoriel on devrait avoir $u_a - u_b = u_c - u_d$.\n",
        "\n",
        "**Exercice** :\n",
        "- Écrire une fonction qui résoud une telle analogie, sachant $a$, $b$ et $c$.\n",
        "- La tester !\n",
        "\n",
        "Cellule de code <x1J-rnsDyCSU>\n",
        "# %% [code]\n",
        "def solve_analogy(word_a, word_b, word_c):\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}