{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdeiGpO73h0V6vvoSO8/7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOMBOU12/Marius/blob/main/NLP_R%C3%A9sum%C3%A9_d'article.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici l'implémentation que j'ai crée qui divise les articles avant de les résumer.\n",
        "\n",
        "Mais ça prend trop de temps pour être exécuter sur les 400 articles du train set OBS.\n",
        "\n",
        "Peut-être l'essayer sur 10 articles ne serait pas une mauvaise idée.\n",
        "\n",
        "Ici le modèle est déjà préentrainé.\n"
      ],
      "metadata": {
        "id": "_qQU5LBehGz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hUy-m5qg8I4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Charger le modèle LED pour résumer des textes longs\n",
        "model_name = \"allenai/led-large-16384\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Dossier contenant les fichiers d'articles\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "\n",
        "# Fonction pour lire un fichier\n",
        "def lire_article(chemin_dossier, nom_fichier):\n",
        "    chemin_complet = os.path.join(chemin_dossier, nom_fichier)\n",
        "    with open(chemin_complet, 'r', encoding='utf-8') as fichier:\n",
        "        return fichier.read()\n",
        "\n",
        "# Fonction pour résumer un texte long\n",
        "def resumer_texte_long(texte, max_length=512, chunk_size=4096):\n",
        "    # Diviser le texte en morceaux\n",
        "    morceaux = [texte[i:i + chunk_size] for i in range(0, len(texte), chunk_size)]\n",
        "    resumes = []\n",
        "    for morceau in morceaux:\n",
        "        # Générer un résumé pour chaque morceau\n",
        "        inputs = tokenizer(morceau, return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=50, length_penalty=2.0)\n",
        "        resumes.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "    # Combiner les résumés partiels en un seul texte\n",
        "    texte_final = \" \".join(resumes)\n",
        "    return texte_final\n",
        "\n",
        "# Lister tous les fichiers d'articles\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Liste pour stocker les résumés générés\n",
        "resumes_generes = []\n",
        "\n",
        "# Traiter chaque fichier d'article\n",
        "for article_fichier in fichiers_articles:\n",
        "    try:\n",
        "        # Lire le contenu de l'article\n",
        "        contenu_article = lire_article(dossier_articles, article_fichier)\n",
        "\n",
        "        # Résumer l'article\n",
        "        resume = resumer_texte_long(contenu_article, max_length=512, chunk_size=4096)\n",
        "\n",
        "        # Ajouter le résumé et l'identifiant à la liste\n",
        "        resumes_generes.append({\n",
        "            \"fichier\": article_fichier,\n",
        "            \"resume\": resume\n",
        "        })\n",
        "\n",
        "        # Afficher un message de progression\n",
        "        print(f\"Résumé généré pour : {article_fichier}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour l'article {article_fichier}: {e}\")\n",
        "\n",
        "# Sauvegarder les résumés dans un fichier CSV\n",
        "df_resumes = pd.DataFrame(resumes_generes)\n",
        "df_resumes.to_csv(\"resumes_articles.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Tous les résumés ont été sauvegardés dans 'resumes_articles.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t2ErluZSVWxs"
      }
    }
  ]
}