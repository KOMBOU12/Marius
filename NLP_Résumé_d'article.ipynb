{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7V+iSd4s4hzDcR3T9CQcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOMBOU12/Marius/blob/main/NLP_R%C3%A9sum%C3%A9_d'article.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici l'implémentation que j'ai crée qui divise les articles avant de les résumer.\n",
        "\n",
        "Mais ça prend trop de temps pour être exécuter sur les 400 articles du train set OBS.\n",
        "\n",
        "Peut-être l'essayer sur 10 articles ne serait pas une mauvaise idée.\n",
        "\n",
        "Ici le modèle est déjà préentrainé.\n"
      ],
      "metadata": {
        "id": "_qQU5LBehGz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hUy-m5qg8I4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Charger le modèle LED pour résumer des textes longs\n",
        "model_name = \"allenai/led-large-16384\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Dossier contenant les fichiers d'articles\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "\n",
        "# Fonction pour lire un fichier\n",
        "def lire_article(chemin_dossier, nom_fichier):\n",
        "    chemin_complet = os.path.join(chemin_dossier, nom_fichier)\n",
        "    with open(chemin_complet, 'r', encoding='utf-8') as fichier:\n",
        "        return fichier.read()\n",
        "\n",
        "# Fonction pour résumer un texte long\n",
        "def resumer_texte_long(texte, max_length=512, chunk_size=4096):\n",
        "    # Diviser le texte en morceaux\n",
        "    morceaux = [texte[i:i + chunk_size] for i in range(0, len(texte), chunk_size)]\n",
        "    resumes = []\n",
        "    for morceau in morceaux:\n",
        "        # Générer un résumé pour chaque morceau\n",
        "        inputs = tokenizer(morceau, return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=50, length_penalty=2.0)\n",
        "        resumes.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "    # Combiner les résumés partiels en un seul texte\n",
        "    texte_final = \" \".join(resumes)\n",
        "    return texte_final\n",
        "\n",
        "# Lister tous les fichiers d'articles\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Liste pour stocker les résumés générés\n",
        "resumes_generes = []\n",
        "\n",
        "# Traiter chaque fichier d'article\n",
        "for article_fichier in fichiers_articles:\n",
        "    try:\n",
        "        # Lire le contenu de l'article\n",
        "        contenu_article = lire_article(dossier_articles, article_fichier)\n",
        "\n",
        "        # Résumer l'article\n",
        "        resume = resumer_texte_long(contenu_article, max_length=512, chunk_size=4096)\n",
        "\n",
        "        # Ajouter le résumé et l'identifiant à la liste\n",
        "        resumes_generes.append({\n",
        "            \"fichier\": article_fichier,\n",
        "            \"resume\": resume\n",
        "        })\n",
        "\n",
        "        # Afficher un message de progression\n",
        "        print(f\"Résumé généré pour : {article_fichier}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour l'article {article_fichier}: {e}\")\n",
        "\n",
        "# Sauvegarder les résumés dans un fichier CSV\n",
        "df_resumes = pd.DataFrame(resumes_generes)\n",
        "df_resumes.to_csv(\"resumes_articles.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Tous les résumés ont été sauvegardés dans 'resumes_articles.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importationn du modèle préentrainé**"
      ],
      "metadata": {
        "id": "zkNTTBn5YFJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import evaluate\n",
        "import gc\n",
        "\n",
        "hf_token = \"\" #Mettre son token dans Kaggle\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "wWR_oXfAYDf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partie du code pour résumer 10 articles du dossier test et pour obtenir les score**"
      ],
      "metadata": {
        "id": "t2ErluZSVWxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import evaluate\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "articles_dir = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/OBS_test/articles_OBS_test/\"\n",
        "article_files = [f for f in os.listdir(articles_dir) if f.endswith('.txt')]\n",
        "selected_articles = random.sample(article_files, 10)\n",
        "\n",
        "summaries = []\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "for article_file in selected_articles:\n",
        "    article_path = os.path.join(articles_dir, article_file)\n",
        "    with open(article_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        article_text = file.read()\n",
        "\n",
        "\n",
        "    prompt =[\n",
        "    {\"role\":\"system\", \"content\" : \"You are an expert assistant specialized in summarizing scientific articles. Provide a structured summary\"},\n",
        "     {\"role\":\"user\", \"content\" : article_text}]\n",
        "    chat = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
        "\n",
        "    input_ids = tokenizer(chat, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    try:\n",
        "        output_ids = model.generate(\n",
        "            **input_ids, #input_ids[\"input_ids\"],\n",
        "            #attention_mask=input_ids[\"attention_mask\"],\n",
        "            do_sample=True,\n",
        "            top_p=50,  # Réduction pour privilégier les options les plus probables\n",
        "            temperature=1,  # Réduction pour limiter la stochasticité\n",
        "            max_new_tokens=150  # Plus de tokens pour des résumés plus longs\n",
        "        )\n",
        "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        print(f\"Erreur de mémoire pour {article_file}. Réduction des tokens...\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    summaries.append({\"Article_ID\": article_file, \"Summary\": summary})\n",
        "\n",
        "    # Remplacez par un vrai résumé de référence\n",
        "    reference_summary = \"Résumé réel de l'article ici.\"\n",
        "    results = rouge.compute(predictions=[summary], references=[reference_summary])\n",
        "\n",
        "    print(f\"Scores ROUGE pour {article_file}:\")\n",
        "    for key, value in results.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "submission_path = \"/kaggle/working/submission2.csv\"\n",
        "submission2 = pd.DataFrame(summaries)\n",
        "submission2.to_csv(submission_path, index=False)"
      ],
      "metadata": {
        "id": "-XDxSrctXb68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il faut chercher à améliorer les scores...."
      ],
      "metadata": {
        "id": "_P2F5I0eYPMv"
      }
    }
  ]
}