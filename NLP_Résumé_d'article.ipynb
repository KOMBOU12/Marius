{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUZDPi9nCktW38BstAq+iq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOMBOU12/Marius/blob/main/NLP_R%C3%A9sum%C3%A9_d'article.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici l'implémentation que j'ai crée qui divise les articles avant de les résumer.\n",
        "\n",
        "Mais ça prend trop de temps pour être exécuter sur les 400 articles du train set OBS.\n",
        "\n",
        "Peut-être l'essayer sur 10 articles ne serait pas une mauvaise idée.\n",
        "\n",
        "Ici le modèle est déjà préentrainé.\n"
      ],
      "metadata": {
        "id": "_qQU5LBehGz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hUy-m5qg8I4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Charger le modèle LED pour résumer des textes longs\n",
        "model_name = \"allenai/led-large-16384\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Dossier contenant les fichiers d'articles\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "\n",
        "# Fonction pour lire un fichier\n",
        "def lire_article(chemin_dossier, nom_fichier):\n",
        "    chemin_complet = os.path.join(chemin_dossier, nom_fichier)\n",
        "    with open(chemin_complet, 'r', encoding='utf-8') as fichier:\n",
        "        return fichier.read()\n",
        "\n",
        "# Fonction pour résumer un texte long\n",
        "def resumer_texte_long(texte, max_length=512, chunk_size=4096):\n",
        "    # Diviser le texte en morceaux\n",
        "    morceaux = [texte[i:i + chunk_size] for i in range(0, len(texte), chunk_size)]\n",
        "    resumes = []\n",
        "    for morceau in morceaux:\n",
        "        # Générer un résumé pour chaque morceau\n",
        "        inputs = tokenizer(morceau, return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, min_length=50, length_penalty=2.0)\n",
        "        resumes.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "    # Combiner les résumés partiels en un seul texte\n",
        "    texte_final = \" \".join(resumes)\n",
        "    return texte_final\n",
        "\n",
        "# Lister tous les fichiers d'articles\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Liste pour stocker les résumés générés\n",
        "resumes_generes = []\n",
        "\n",
        "# Traiter chaque fichier d'article\n",
        "for article_fichier in fichiers_articles:\n",
        "    try:\n",
        "        # Lire le contenu de l'article\n",
        "        contenu_article = lire_article(dossier_articles, article_fichier)\n",
        "\n",
        "        # Résumer l'article\n",
        "        resume = resumer_texte_long(contenu_article, max_length=512, chunk_size=4096)\n",
        "\n",
        "        # Ajouter le résumé et l'identifiant à la liste\n",
        "        resumes_generes.append({\n",
        "            \"fichier\": article_fichier,\n",
        "            \"resume\": resume\n",
        "        })\n",
        "\n",
        "        # Afficher un message de progression\n",
        "        print(f\"Résumé généré pour : {article_fichier}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour l'article {article_fichier}: {e}\")\n",
        "\n",
        "# Sauvegarder les résumés dans un fichier CSV\n",
        "df_resumes = pd.DataFrame(resumes_generes)\n",
        "df_resumes.to_csv(\"resumes_articles.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Tous les résumés ont été sauvegardés dans 'resumes_articles.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importationn du modèle préentrainé**"
      ],
      "metadata": {
        "id": "zkNTTBn5YFJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import evaluate\n",
        "import gc\n",
        "\n",
        "hf_token = \"\" #Mettre son token dans Kaggle\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ")"
      ],
      "metadata": {
        "id": "wWR_oXfAYDf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Partie du code pour résumer 10 articles du dossier test et pour obtenir les score**"
      ],
      "metadata": {
        "id": "t2ErluZSVWxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import evaluate\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "articles_dir = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/OBS_test/articles_OBS_test/\"\n",
        "article_files = [f for f in os.listdir(articles_dir) if f.endswith('.txt')]\n",
        "selected_articles = random.sample(article_files, 10)\n",
        "\n",
        "summaries = []\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "for article_file in selected_articles:\n",
        "    article_path = os.path.join(articles_dir, article_file)\n",
        "    with open(article_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        article_text = file.read()\n",
        "\n",
        "\n",
        "    prompt =[\n",
        "    {\"role\":\"system\", \"content\" : \"You are an expert assistant specialized in summarizing scientific articles. Provide a structured summary\"},\n",
        "     {\"role\":\"user\", \"content\" : article_text}]\n",
        "    chat = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
        "\n",
        "    input_ids = tokenizer(chat, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    try:\n",
        "        output_ids = model.generate(\n",
        "            **input_ids, #input_ids[\"input_ids\"],\n",
        "            #attention_mask=input_ids[\"attention_mask\"],\n",
        "            do_sample=True,\n",
        "            top_p=50,  # Réduction pour privilégier les options les plus probables\n",
        "            temperature=1,  # Réduction pour limiter la stochasticité\n",
        "            max_new_tokens=150  # Plus de tokens pour des résumés plus longs\n",
        "        )\n",
        "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        print(f\"Erreur de mémoire pour {article_file}. Réduction des tokens...\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    summaries.append({\"Article_ID\": article_file, \"Summary\": summary})\n",
        "\n",
        "    # Remplacez par un vrai résumé de référence\n",
        "    reference_summary = \"Résumé réel de l'article ici.\"\n",
        "    results = rouge.compute(predictions=[summary], references=[reference_summary])\n",
        "\n",
        "    print(f\"Scores ROUGE pour {article_file}:\")\n",
        "    for key, value in results.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "submission_path = \"/kaggle/working/submission2.csv\"\n",
        "submission2 = pd.DataFrame(summaries)\n",
        "submission2.to_csv(submission_path, index=False)"
      ],
      "metadata": {
        "id": "-XDxSrctXb68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vous pouvez essayer le modèle :  **meta-llama/Llama-3.1-8B-Instruct** . Je n'ai plus d'heures de GPU pour continuer.\n",
        "\n",
        "Ce modèle a 8 milliards de paramètres contre 3 milliards pour le modèle précédent."
      ],
      "metadata": {
        "id": "_P2F5I0eYPMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = \" \"\n",
        "\n",
        "# Charger le tokenizer et le modèle\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "# Tester un résumé avec le nouveau modèle\n",
        "for article_file in selected_articles:\n",
        "    article_path = os.path.join(articles_dir, article_file)\n",
        "    with open(article_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        article_text = file.read()\n",
        "\n",
        "    prompt = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert assistant specialized in summarizing scientific articles. Provide a structured summary.\"},\n",
        "        {\"role\": \"user\", \"content\": article_text}\n",
        "    ]\n",
        "    chat = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
        "\n",
        "    input_ids = tokenizer(chat, add_special_tokens=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    try:\n",
        "        output_ids = model.generate(\n",
        "            **input_ids,\n",
        "            do_sample=True,\n",
        "            top_p=0.8,\n",
        "            temperature=0.5,\n",
        "            max_new_tokens=200  # Alloué plus de tokens pour des résumés plus longs\n",
        "        )\n",
        "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        print(f\"Erreur de mémoire pour {article_file}. Réduction des tokens...\")\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    summaries.append({\"Article_ID\": article_file, \"Summary\": summary})\n",
        "\n",
        "    # ROUGE : à évaluer avec un vrai résumé de référence\n",
        "    reference_summary = \"Résumé réel de l'article ici.\"\n",
        "    results = rouge.compute(predictions=[summary], references=[reference_summary])\n",
        "\n",
        "    print(f\"Scores ROUGE pour {article_file}:\")\n",
        "    for key, value in results.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Sauvegarder les résultats\n",
        "submission_path = \"/kaggle/working/submission8B.csv\"\n",
        "submission = pd.DataFrame(summaries)\n",
        "submission.to_csv(submission_path, index=False)\n"
      ],
      "metadata": {
        "id": "fsOJYXiKPY1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Le fameux BART !!!!**"
      ],
      "metadata": {
        "id": "s5l9N2QUi3tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette partie initialise les composants nécessaires pour générer des résumés avec BART."
      ],
      "metadata": {
        "id": "2DeiEsHjjQGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datasets import Dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "\n",
        "# Charge le modèle BART et le tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "gDUxv1Ypi9yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on utilise le modèle préentraîné BART pour générer des résumés à partir d'articles scientifiques, on les compare avec des résumés de référence, et on calcule le score ROUGE-2 pour évaluer la qualité des résumés générés."
      ],
      "metadata": {
        "id": "Z-GoarIxjk4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Charger le modèle BART et le tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Chemins vers les dossiers des articles et des résumés de référence\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "dossier_resumes = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\n",
        "\n",
        "# Charger les articles et leurs résumés de référence\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "articles = {}\n",
        "resumes_reference = {}\n",
        "\n",
        "def lire_fichier(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(filepath, 'r', encoding='latin-1') as f:\n",
        "            return f.read()\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = lire_fichier(os.path.join(dossier_articles, fichier))\n",
        "    resumes_reference[identifiant] = lire_fichier(os.path.join(dossier_resumes, f\"abstract-{identifiant}.txt\"))\n",
        "\n",
        "# Sélectionner 5 articles aléatoires\n",
        "random.seed(42)  # Pour garantir la reproductibilité\n",
        "selected_ids = random.sample(list(articles.keys()), 5)\n",
        "\n",
        "# Stocker les articles, résumés générés et scores\n",
        "resumes_generes = []\n",
        "rouge_scores = []\n",
        "\n",
        "# Fonction pour résumer un texte avec BART\n",
        "def resumer_texte(texte, max_length=130, min_length=30, length_penalty=2.0, num_beams=4):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + texte, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        length_penalty=length_penalty,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Calcul des résumés et des scores ROUGE-2\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge2\"], use_stemmer=True)\n",
        "\n",
        "for identifiant in selected_ids:\n",
        "    article = articles[identifiant]\n",
        "    resume_reference = resumes_reference[identifiant]\n",
        "\n",
        "    # Générer le résumé\n",
        "    resume_genere = resumer_texte(article)\n",
        "    resumes_generes.append((identifiant, resume_genere))\n",
        "\n",
        "    # Calculer le score ROUGE-2\n",
        "    scores = scorer.score(resume_reference, resume_genere)\n",
        "    rouge_scores.append(scores[\"rouge2\"].fmeasure)\n",
        "\n",
        "# Afficher les résultats\n",
        "for i, (identifiant, resume_genere) in enumerate(resumes_generes):\n",
        "    print(f\"Article ID: {identifiant}\")\n",
        "    print(f\"Résumé généré: {resume_genere}\")\n",
        "    print(f\"ROUGE-2 Score: {rouge_scores[i]:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Afficher le score ROUGE-2 moyen\n",
        "avg_rouge2 = sum(rouge_scores) / len(rouge_scores)\n",
        "print(\"Score ROUGE-2 moyen:\")\n",
        "print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n"
      ],
      "metadata": {
        "id": "cSifrDrLjsTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a une moyenne de score de 0,04. **BART** a une limite de 1024 tokens. Les articles plus longs sont tronqués, ce qui peut entraîner une perte d'informations essentielles. Les résumés générés pourraient être incomplets ou manquer de cohérence."
      ],
      "metadata": {
        "id": "Y5dxleeZjzXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Modèle utilisant un Prompt, score moyen de 0,12 sur 10 articles**\n",
        "\n",
        "Ce modèle utilise un prompt, prendes articles les resummes , ensuite calcul le Score Rouge 2 pour chaque article, somme les scores et donne la moyenne."
      ],
      "metadata": {
        "id": "x1Nmho4ZxChV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import evaluate\n",
        "import gc\n",
        "\n",
        "# Charger le tokenizer et le modèle\n",
        "hf_token = \"\" #Mettre ton Token à toi\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", token=hf_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "# Chemins des articles et résumés de référence\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "dossier_resumes = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\n",
        "\n",
        "# Charger les fichiers\n",
        "article_files = [f for f in os.listdir(dossier_articles) if f.endswith('.txt')]\n",
        "selected_articles = random.sample(article_files, 10)\n",
        "\n",
        "# Charger la métrique ROUGE\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Calculer les scores ROUGE-2\n",
        "rouge_scores = []\n",
        "\n",
        "for article_file in selected_articles:\n",
        "    article_path = os.path.join(dossier_articles, article_file)\n",
        "    identifiant = article_file.split(\"-\")[1].split(\".\")[0]\n",
        "    resume_path = os.path.join(dossier_resumes, f\"abstract-{identifiant}.txt\")\n",
        "\n",
        "    try:\n",
        "        # Lire l'article et le résumé de référence\n",
        "        with open(article_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            article_text = file.read()\n",
        "        with open(resume_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            reference_summary = file.read()\n",
        "\n",
        "        # Préparer l'invite pour le modèle\n",
        "        prompt = (\n",
        "            f\"System: You are an expert assistant. Provide a clear, concise summary for the following scientific article.\\n\"\n",
        "            f\"Article:\\n{article_text[:3000]}\\nSummary:\"\n",
        "        )\n",
        "\n",
        "        # Tokenisation de l'invite\n",
        "        input_ids = tokenizer(prompt, add_special_tokens=True, return_tensors=\"pt\", truncation=True, max_length=1024).to(\"cuda\")\n",
        "\n",
        "        # Génération du résumé\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids[\"input_ids\"],\n",
        "            attention_mask=input_ids[\"attention_mask\"],\n",
        "            do_sample=True,\n",
        "            top_p=0.85,\n",
        "            temperature=0.5,\n",
        "            max_new_tokens=300\n",
        "        )\n",
        "        generated_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Calcul du score ROUGE-2\n",
        "        results = rouge.compute(predictions=[generated_summary], references=[reference_summary])\n",
        "        rouge_scores.append(results[\"rouge2\"])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour l'article {article_file}: {e}\")\n",
        "\n",
        "    # Libérer la mémoire GPU\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Afficher les scores ROUGE-2\n",
        "print(\"Scores ROUGE-2 pour les articles sélectionnés :\")\n",
        "for score in rouge_scores:\n",
        "    print(f\"ROUGE-2: {score:.4f}\")\n",
        "\n",
        "# Calculer et afficher la moyenne\n",
        "avg_rouge2 = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
        "print(f\"Score ROUGE-2 moyen : {avg_rouge2:.4f}\")\n"
      ],
      "metadata": {
        "id": "UpOf-eUExAat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MISE A JOUR DU MODELE BART**"
      ],
      "metadata": {
        "id": "VYuT8IFeeAJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le moèle précedent les appariements n'ont pas été pris en compte."
      ],
      "metadata": {
        "id": "6Blnq8fieIHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datasets import Dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "\n",
        "# Charge le modèle BART et le tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "QS2pxNPxeHBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialisation du modèle et du tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Chemins vers les dossiers des articles et résumés\n",
        "dossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "\n",
        "# Fonction pour lire un fichier avec gestion des erreurs d'encodage\n",
        "def lire_fichier(chemin_fichier):\n",
        "    try:\n",
        "        with open(chemin_fichier, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(chemin_fichier, \"r\", encoding=\"latin-1\") as f:\n",
        "            return f.read()\n",
        "\n",
        "# Fonction pour générer un résumé avec BART\n",
        "def resumer_texte(texte, max_length=130, min_length=30, length_penalty=2.0, num_beams=4):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + texte, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        length_penalty=length_penalty,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Initialisation du scoreur ROUGE\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge2\"], use_stemmer=True)\n",
        "\n",
        "# Application du modèle et calcul des scores ROUGE-2\n",
        "rouge_scores = []\n",
        "for i in range(5):  # Lire les 5 premiers articles\n",
        "    identifiant = dataset[i][\"identifiant\"]\n",
        "    fichier_article = dataset[i][\"article\"]\n",
        "    fichier_abstract = dataset[i][\"abstract\"]\n",
        "\n",
        "    # Chemins complets des fichiers\n",
        "    chemin_fichier_article = os.path.join(dossier_articles, fichier_article)\n",
        "    chemin_fichier_abstract = os.path.join(dossier_abstracts, fichier_abstract)\n",
        "\n",
        "    # Lire le contenu des fichiers\n",
        "    contenu_article = lire_fichier(chemin_fichier_article)\n",
        "    contenu_abstract = lire_fichier(chemin_fichier_abstract)\n",
        "\n",
        "    # Générer le résumé avec le modèle\n",
        "    resume_genere = resumer_texte(contenu_article)\n",
        "\n",
        "    # Calculer le score ROUGE-2\n",
        "    rouge_score = scorer.score(contenu_abstract, resume_genere)[\"rouge2\"].fmeasure\n",
        "    rouge_scores.append(rouge_score)\n",
        "\n",
        "    # Afficher les résultats pour cet article\n",
        "    print(f\"Article ID: {identifiant}\")\n",
        "    print(f\"Résumé généré:\\n{resume_genere}\")\n",
        "    print(f\"Résumé réel:\\n{contenu_abstract}\")\n",
        "    print(f\"Score ROUGE-2: {rouge_score:.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Calculer et afficher le score ROUGE-2 moyen\n",
        "avg_rouge2 = sum(rouge_scores) / len(rouge_scores)\n",
        "print(f\"Score ROUGE-2 moyen sur 5 articles : {avg_rouge2:.4f}\")\n"
      ],
      "metadata": {
        "id": "Giqy0bv2ebxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le score est toujours nul mais légèrement en augmentation, comme quoi les appraiements ont un impacte."
      ],
      "metadata": {
        "id": "ErgoporoedtE"
      }
    }
  ]
}